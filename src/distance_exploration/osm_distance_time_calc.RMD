---
title: "OSM Distance-Time Calculation"
author: "Quinton"
date: "7/11/2019"
output: 
  github_document: default
  html_document: default
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
#Setting root directory
knitr::opts_knit$set(echo = TRUE,
                     root.dir = rprojroot::find_rstudio_root_file())

#Load Pacman for multiple package loads
if (!require("pacman")) install.packages(pacman)

#Need most updated dplyr if haven't already done this
#install.packages("dplyr") #need updated dplyr for sf objects

#Load all the good stuff
pacman::p_load(tidyverse, purrr, sf, mapview, ggmap,
               patchwork, osmdata, mapview, traveltime,
               iterators, doParallel, foreach, parallel,
               geosphere)

#Controlling figure output in markdown
knitr::opts_chunk$set(
#  fig.height =   
  fig.width = 6,
#  fig.asp = .5,
  out.width = "90%",
#  out.height = 
  cache = TRUE
)

#Set Theme for ggplot2
theme_set(theme_bw() + theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom"))

#Set Scientific notation output for knitr
options(scipen = 999999)
```


##1. Read OSM Final Data  

####a. Introduction

```{r warning = FALSE, message = FALSE}
osm.df    <- read_csv("./data/working/OSM_joined/7_10_2019_osm_joined.csv")
osm.dim   <- dim(osm.df)
osm.names <- names(osm.df) %>% paste0(., collapse = ", ")

#Number of obs. for each variable
table(osm.df$variable)
```

The dimensions of the raw data are `r osm.dim %>% paste0(., ", ")` and the variables contained are named `r osm.names`. 

####b. Sample Park Boundary  

Reading in the final osm data. Need to reduce the number of points to look through for the park polygons. Using centroids is definitely going to bias the distance-time coverage downward. Further, we are interested in whether they are close to the boundary. However, we don't need to look through every point. Instead we take a random sample of boundary points for each polygon and use those instead. 

This is a simplification for computational efficiency, but with random sampling, the only error should really be random. From a uniform distribution, we should get a relatively good sample of the boundary points to calculate distance_time. 

```{r, warning = FALSE, message = FALSE}
#Investigating the Number of Points in each Park Boundary
park.df <- read_csv("./data/working/OSM_joined/7_10_2019_osm_joined.csv") %>%
  nest(-c(environment, variable)) %>%
  filter(variable == "Park") %>%
  dplyr::select(data) %>%
  unnest() %>%
  nest(-object_id) %>%
  mutate(
    length = map_dbl(data, nrow)
  ) 
#Check out length of Parks
park.df$length %>% summary()

#Distribution
park.df %>%
  ggplot(aes(x = length)) +
  geom_histogram(fill = "purple", colour = "black") +
  labs(
    x = "Length of Polygon Boundary",
    y = "Count",
    title = "Distribution of Park Boundary Length"
  )

#Actual unique number of parks
nrow(park.df)
```


As the variability in length (analogous to the size of park) is quite high, quite a few small parks and quite a few extremely large parks. As such we will take a random sample that is proportionate to the size of the park. To do say we say we will sample 25% of the boundary points at random.   


```{r warning = FALSE, message = FALSE}
#Mutating/sampling the spatial data by park object_id
park.samp.df <- park.df %>%
  mutate(
    data = map(.x = data, 
               ~ slice(.x, sample(1:nrow(.x), (nrow(.x)/5) %>% ceiling(), replace = FALSE))
               ),
    length_new = map_dbl(data, nrow)
  )

#Check out new length of Parks
park.samp.df$length_new %>% summary()

#Distribution
park.samp.df %>%
  ggplot(aes(x = length_new)) +
  geom_histogram(fill = "purple", colour = "black") +
  labs(
    x = "Length of Polygon Boundary",
    y = "Count",
    title = "Distribution of Park Boundary Length"
  )

```

####c. Alternatively make a Park centroid variable

```{r}
#Mutating/sampling the spatial data by park object_id
park.cent.df <- park.df %>%
  mutate(
    data = map(.x = data, 
               ~ ifelse(nrow(data) > 2,
                        centroid(.x) %>%
                        as_tibble(),
                        data %>%
                          slice(1)
                       )
               )
  ) %>% unnest()
```

####d. Replace Park Data in OSM Final Data  (either centroid or sample)

```{r}
osm.df <- osm.df %>%
  nest(-c(environment, variable)) %>%
  mutate(
    data = ifelse(variable == "Park", 
                  park.samp.df %>%  #Choose which park .df
                  dplyr::select(object_id, data) %>%
                  unnest(),
                  data),
    data = map(data, as_tibble)
  )
```

####e. For Now, filter out park; will deal with later
```{r}
osm.df <- osm.df %>%
  filter(variable != "Park") %>%
  unnest()

nrow(osm.df)
```


#2. Calculate Distance Time

```{r eval = FALSE}
#Cong
#App ID: 3c54476f
#Key: 049992edda691331bc5bbe90dd7c6952

#Mine
#Application ID: 12251934
#API key:  b5fb14aa79e0278e065d9de3ef95dbd9

#Gather API keys and ID's for Parallelizing
api.grid <- 
  tibble(
    api_keys = c("19bce8c3fc18e723a1315150fa56e8ab",
                 "9e0866ac4ca48ff473d284caa2cb1f04",
                 "83650a17db06975f9a290458275b1788",
                 "76305fba3585306f803584fd9c1836e4",
                 "499f74892de77d9b8712f309ec1c6668",
                 "a258ee1b2888bb5d1fbc6b96af0008bb",
                 "ccc4a7aea2d45d77c1090773d259ae79",
                 "f3432d6da9070b29dc87dd21473bf2b2"
              ) %>%
                  rep(., times = ceiling(nrow(osm.df)/8)),
    api_id  = c("1f5210c1",
                "f5476b5a",
                "18d32e60",
                "c3f5bc98",
                "fca92235",
                "224b4181",
                "a32fa90b",
                "20dc688b"
              ) %>%
                  rep(., times = ceiling(nrow(osm.df)/8))
  )

api.grid <- api.grid %>%
  slice(1:nrow(osm.df))

osm.df <- bind_cols(osm.df, api.grid)

#New input
                               #  traveltime=1200,
                               #  type="public_transport",
                               #  departure="2019-6-27T08:00:00Z")

#Objective Task Function
taskFun <- function(data, i) {
  Sys.sleep(6.05)
  
  traveltime_map(appId      = data$api_id[i],
                 apiKey     = data$api_keys[i],
                 location   = c(data$latitude[i], data$longitude[i]),
                 traveltime = 1200,
                 type       = "public_transport",
                 departure  = "2019-6-27T08:00:00Z")
}

#Nest to save .RDS each time we finish an object
osm.df <- osm.df %>%
  nest(-c(environment, variable))

#Parallelize
#detectCores()
nCores <- 8
registerDoParallel(nCores)

#Initalize result list (each variable)
result <- list()

#Iterate over all variables (so we can store and save as we go *doesn't store object otherwise)

for(j in 1:nrow(osm.df)) {
  
#Count start time for each iteration  
#a <- Sys.time()

#1 X nrow(osm.df$data) iterations 
result[[j]] <- foreach(i = 1:nrow(osm.df$data[[j]]),
                  .packages = "traveltime") %dopar% {
  outSub <- taskFun(osm.df$data[[j]], i)
  outSub
} 

#Count end time
#b <- Sys.time()
#print(sprintf("Iteration %i took %f seconds", j, (b - a)))
print(sprintf("Finished %s iteration", osm.df$variable[j]))

write_rds(result[[j]], sprintf("./data/working/Time_distance_files/public_transport_osm_files/%s.rds", 
                               osm.df$variable[j] %>% janitor::make_clean_names()))
}
```

